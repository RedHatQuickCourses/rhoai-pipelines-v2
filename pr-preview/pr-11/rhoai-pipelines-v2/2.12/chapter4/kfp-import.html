<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kubeflow Pipelines SDK :: RHOAI Data Science Pipelines</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">RHOAI Data Science Pipelines</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-pipelines-v2" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">RHOAI Data Science Pipelines</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/minio-install.html">MinIO S3 Compatible Storage Setup</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/dsp-intro.html">Introduction to ML Pipelines</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/dsp-concepts.html">Data Science Pipeline Concepts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Data Science Pipelines</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/data-science-pipeline-app.html">Data Science Pipeline Applications</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhoai-resources.html">OpenShift AI Resources</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Creating Pipelines with Elyra</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/elyra-pipelines.html">Elyra Pipelines</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">KFP SDK</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="kfp-import.html">Kubeflow Pipelines SDK</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">blah blah blah 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix A</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">RHOAI Data Science Pipelines</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">RHOAI Data Science Pipelines</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">RHOAI Data Science Pipelines</a></li>
    <li><a href="index.html">KFP SDK</a></li>
    <li><a href="kfp-import.html">Kubeflow Pipelines SDK</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Kubeflow Pipelines SDK</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In the previous section, we created a two task pipeline by using the Elyra GUI pipeline editor. The kfp SDK provides a Python API for creating pipelines. The SDK is available as a Python package that you can install by using the <code>pip install kfp</code> command. With this package, you can use Python code to create a pipeline and then compile it to YAML format. Then we can import the YAML code into OpenShift AI.</p>
</div>
<div class="paragraph">
<p>This course does not delve into the details of how to use the SDK. Instead, it provides the files for you to view and upload.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_exercise_importing_a_data_science_pipeline"><a class="anchor" href="#_lab_exercise_importing_a_data_science_pipeline"></a>Lab Exercise: Importing a Data Science Pipeline</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h3>
<div class="ulist">
<ul>
<li>
<p>Continue to use the <code>fraud-detection</code> Data Science Project that you created in the previous section. We won&#8217;t need a workbench in this section, but you should have completed all lab exercises up through Data Science Pipelines / OpenShift AI Resources section of the course in order for the environment to support pipeline creation.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/import_pipeline_yaml.gif" alt="import pipeline yaml" width="600">
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the RHOAI side navigation menu, click `Data Science Pipelines</p>
</li>
<li>
<p>Select the import pipeline button</p>
</li>
<li>
<p>For the pipeline name use: fraud-detection-example</p>
</li>
<li>
<p>Select the import by url option</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">https://github.com/RedHatQuickCourses/rhoai-pipelines-v2/blob/main/downloads/fraud_detection.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Select the Import Pipeline button at the bottom.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Once the pipeline definition is imported, it can be viewed in the data science pipeline dashboard.</p>
</div>
<div class="paragraph">
<p>There are three tabs in DSP dashboard:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Graph - the DAG view of the pipeline defined</p>
</li>
<li>
<p>Summary - shows the pipeline spec and version IDs</p>
</li>
<li>
<p>Pipeline spec - is the import yaml file that defined the pipeline.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The pipeline is now available to be executed, but currently there have been no <strong>one-off or scheduled runs</strong> for this pipeline, it has only been defined in the system.</p>
</div>
</div>
<div class="sect2">
<h3 id="_experiments_and_runs"><a class="anchor" href="#_experiments_and_runs"></a>Experiments And Runs</h3>
<div class="paragraph">
<p>An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize pipeline runs into logical groups. Experiments can contain arbitrary runs, including recurring runs.</p>
</div>
<div class="paragraph">
<p>A run can be configured using the Data Science Pipeline UI or programmatically using the <em>KFP SDK</em>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/create_pipeline_run.gif" alt="create pipeline run">
</div>
</div>
<div class="paragraph">
<p>To create a <em>run</em> for the fraud-detection-example pipeline we just imported.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Locate the fraud-detection-example in the data science pipeline menu / dashboard.</p>
</li>
<li>
<p>Use the menu at the far right to:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>upload a new version of the pipeline</p>
</li>
<li>
<p>create a schedule run for sometime in the future</p>
</li>
<li>
<p>create a one-off run</p>
</li>
</ol>
</div>
</li>
<li>
<p>Select the option to create a new run.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To execute the run, we need to imput some information:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run Type: Scheduled runs are exectued from different dashboard - skip this step</p>
</li>
<li>
<p>Define the project and experiment name:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>The project name is immutable and cannot be changed at this time.</p>
</li>
<li>
<p>For the experiment, choose an existing experiment from the list or create a new one.  Select create a new experiment.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Run Details:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Specify a name for this run</p>
</li>
<li>
<p>Add a description</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>The final section describes the pipeline</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Select the pipeline to Run from the drop down</p>
</li>
<li>
<p>Select the version of the pipeline if available.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Depending on Pipeline Definition, some parameters must be specified at runtime.</p>
</div>
<div class="paragraph">
<p>In this case there are two required parameters, which allow this pipeline to have different inputs.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The first parameter is a url location of the data file to be imported during the Run.</p>
</li>
<li>
<p>The second paramenter is number of epochs.
<br>
 [NOTE]
This epoch&#8217;s number is an important hyperparameter for the algorithm. It specifies the number of epochs or complete passes of the entire training dataset passing through the training or learning process of the algorithm.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Select <em>Create</em> Run to start the run.</p>
</div>
<div class="listingblock">
<div class="content">
<pre></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pipeline_parameter_passing"><a class="anchor" href="#_pipeline_parameter_passing"></a>Pipeline Parameter Passing</h3>
<div class="paragraph">
<p>As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.</p>
</div>
<div class="sect3">
<h4 id="_input_parameters"><a class="anchor" href="#_input_parameters"></a>Input Parameters</h4>
<div class="ulist">
<ul>
<li>
<p>Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.</p>
</li>
<li>
<p>Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_output_parameters"><a class="anchor" href="#_output_parameters"></a>Output Parameters</h4>
<div class="ulist">
<ul>
<li>
<p>Output values are returned via files.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_passing_parameters_via_files"><a class="anchor" href="#_passing_parameters_via_files"></a>Passing Parameters via Files</h4>
<div class="paragraph">
<p>To pass an input parameter as a file, the function argument needs to be annotated using the <em>InputPath</em> annotation.
For returning data from a step as a file, the function argument needs to be annotated using the <em>OutputPath</em> annotation.</p>
</div>
<div class="paragraph">
<p><strong>In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.</strong></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There are other parameter annotations available to handle specialised file types
such as <em>InputBinaryFile</em>, <em>OutputBinaryFile</em>.</p>
</div>
<div class="paragraph">
<p>The full annotation list is in the <a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html" target="_blank" rel="noopener">KFP component documentation</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_returning_multiple_values_from_a_task"><a class="anchor" href="#_returning_multiple_values_from_a_task"></a>Returning multiple values from a task</h4>
<div class="paragraph">
<p>If you return a single small value from your component using the <em>return</em> statement, the output parameter is named <strong><em>output</em></strong>.
It is, however, possible to return multiple small values using the Python <em>collection</em> library method <em>namedtuple</em>.</p>
</div>
<div class="paragraph">
<p>From a <a href="https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb" target="_blank" rel="noopener">Kubeflow pipelines example</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def produce_two_small_outputs() -&gt; NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])</code></pre>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If the argument name ends with <em>path and the argument is annotated as an _kfp.components.InputPath</em> or <em>kfp.components.OutputPath</em>, the parameter name is the argument name with the trailing _path removed.</p>
</li>
<li>
<p>If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.</p>
</li>
<li>
<p>If you return a single small value from your component using the return statement, the output parameter is named <strong>output</strong>.</p>
</li>
<li>
<p>If you return several small values from your component by returning a <em>collections.namedtuple</em>, the SDK uses the tuple’s field names as the output parameter names.</p>
</li>
<li>
<p>Otherwise, the SDK uses the argument name as the parameter name.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In the Argo Yaml definition you can see the definition of the <em>input and output artifacts</em>. This can be useful for debugging purposes.</p>
</div>
<div class="paragraph">
<p>You can also see the locations of data stored into the S3 bucket e.g. <em>artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz</em></p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_execution_on_openshift"><a class="anchor" href="#_execution_on_openshift"></a>Execution on OpenShift</h3>
<div class="paragraph">
<p>To enable the <em>pipeline</em> to run on OpenShift we need to pass it the associated <em>kubernetes</em> resources</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>volumes</em></p>
</li>
<li>
<p><em>environment variables</em></p>
</li>
<li>
<p><em>node selectors, taints and tolerations</em></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_volumes"><a class="anchor" href="#_volumes"></a>Volumes</h4>
<div class="paragraph">
<p>Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.</p>
</div>
<div class="paragraph">
<p>Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="paragraph">
<p>The volumes are mounted into the containers using the <strong><em>add_pvolumes</em></strong> method:</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
</div>
<div class="sect3">
<h4 id="_environment_variables"><a class="anchor" href="#_environment_variables"></a>Environment Variables</h4>
<div class="paragraph">
<p>Environment variables can be added to the pod using the <strong><em>add_env_variable</em></strong> method.</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <strong><em>env_from_secret</em></strong> utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the <em>AWS_ACCESS_KEY_ID</em> value is extracted from the <em>s3-secret</em> secret and added to the container defintion as the <em>s3_access_key</em> environment variable.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_node_selectors_taints_and_tolerations"><a class="anchor" href="#_node_selectors_taints_and_tolerations"></a>Node Selectors, Taints and Tolerations</h4>
<div class="paragraph">
<p>Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality.</p>
</div>
<div class="paragraph">
<p>In our example we&#8217;re using the nodes with an attached GPU to execute the step. To do this we need to:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the requisite toleration:</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add the <em>toleration</em> to the pod and add a <em>node selector</em> constraint.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You could also use this approach to ensure that pods without GPU needs are <strong>not</strong> scheduled to nodes with GPUs.</p>
</div>
<div class="paragraph">
<p>For global pipeline pod settings take a look at the <strong><em>PipelineConf</em></strong> class in the '<a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'" target="_blank" rel="noopener">KFP SDK Documentation</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We have only covered a <em>subset</em> of what&#8217;s possible with the <em>KFP SDK</em>.</p>
</div>
<div class="paragraph">
<p>It is also possible to customize significant parts of the <em>pod spec</em> definition with:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Init and Sidecar Pods</p>
</li>
<li>
<p>Pod affinity rules</p>
</li>
<li>
<p>Annotations and labels</p>
</li>
<li>
<p>Retries and Timeouts</p>
</li>
<li>
<p>Resource requests and limits</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>See the the <a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html" target="_blank" rel="noopener">KFP SDK Documentation</a> for more details.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pipeline_execution"><a class="anchor" href="#_pipeline_execution"></a>Pipeline Execution</h3>
<div class="sect3">
<h4 id="_submitting_a_pipeline_and_triggering_a_run"><a class="anchor" href="#_submitting_a_pipeline_and_triggering_a_run"></a>Submitting a Pipeline and Triggering a run</h4>
<div class="paragraph">
<p>The following code demonstrates how to submit and trigger a pipeline run from a <em>Red Hat OpenShift AI WorkBench</em>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if __name__ == '__main__':
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_externally_triggering_a_dsp_pipeline_run"><a class="anchor" href="#_externally_triggering_a_dsp_pipeline_run"></a>Externally Triggering a DSP pipeline run</h4>
<div class="paragraph">
<p>In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>File added to S3 bucket.</p>
</li>
<li>
<p>S3 triggers the send of a webhook payload to an <em>OCP Serverless</em> function.</p>
</li>
<li>
<p>The <em>Serverless</em> function parses the payload and invokes the configured <em>DSP pipeline</em>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>We&#8217;re not going to go through the code and configuration for this, but here is the code to trigger the pipeline.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$dsp_trigger.py[]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The full code is <a href="#attachment$dsp_trigger.py" class="xref unresolved">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <em>pipeline</em> needs to have already been submitted to the DSP runtime.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_data_handling_in_data_science_pipelines"><a class="anchor" href="#_data_handling_in_data_science_pipelines"></a>Data Handling in Data Science Pipelines</h2>
<div class="sectionbody">
<div class="paragraph">
<p>DSP have two sizes of data, conveniently named <strong><em>Small Data</em></strong> and <strong><em>Big Data</em></strong>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><em>Small Data</em> is considered anything that can be passed as a <em>command line argument</em> for example <em>Strings</em>, <em>URLS</em>, <em>Numbers</em>. The overall size should not exceed a few <em>kilobytes</em>.</p>
</li>
<li>
<p>Unsurprisingly, everything else is considered <em>Big Data</em> and should be passed as files.</p>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_handling_large_data_sets"><a class="anchor" href="#_handling_large_data_sets"></a>Handling large data sets</h3>
<div class="paragraph">
<p>DSP support two methods by which to pass large data sets aka <em>Big Data</em> between pipeline steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong><em>Argo Workspaces</em></strong>.</p>
</li>
<li>
<p><strong><em>Volume based data passing method</em></strong>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Data Science Projects <strong><em>Data Connection</em></strong> S3 storage is used to store <em>Output Artifacts</em> and <em>Parameters</em> of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_volume_based_data_passing_method"><a class="anchor" href="#_volume_based_data_passing_method"></a>Volume-based data passing method</h3>
<div class="paragraph">
<p>This approach uses a pre-created OpenShift storage volume (aka <em>PVC</em>) to pass data between the pipeline steps.
An example of this is in the <a href="https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py" target="_blank" rel="noopener">KFP compiler tests</a> which we will discuss here.</p>
</div>
<div class="paragraph">
<p>First create the volume to be used and assign it to a variable:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then add definition to the <em>pipeline configuration</em>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/kfp-import.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <strong><em>data-volume PVC claim</em></strong> needs to exist in the OpenShift namespace while running the pipeline, else the <em>pipeline execution pod</em> fails to deploy and the run terminates.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To pass big data using cloud provider volumes, it&#8217;s recommended to use the <strong><em>volume-based data passing method</em></strong>.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">KFP SDK</a></span>
  <span class="next"><a href="section1.html">blah blah blah 2</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
