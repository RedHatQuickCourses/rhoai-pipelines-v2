<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>blah blah blah 2 :: RHOAI Data Science Pipelines</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">RHOAI Data Science Pipelines</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-pipelines-v2" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">RHOAI Data Science Pipelines</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/minio-install.html">MinIO S3-Compatible Storage Setup</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/dsp-intro.html">Introduction to ML Pipelines</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/dsp-concepts.html">Data Science Pipeline Concepts</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Managing Data Science Pipelines</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/data-science-pipeline-app.html">Data Science Pipeline Applications</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhoai-resources.html">OpenShift AI Resources</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Creating Pipelines with Elyra</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/elyra-pipelines.html">Elyra Pipelines</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">KFP SDK</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kfp-import.html">Kubeflow Pipelines SDK</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix - Lab Examples</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">RHOAI Data Science Pipelines</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">RHOAI Data Science Pipelines</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">RHOAI Data Science Pipelines</a></li>
    <li><a href="section1.html">blah blah blah 2</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">blah blah blah 2</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Overview
To submit a pipeline for execution, you must compile it to YAML with the KFP SDK compiler.</p>
</div>
<div class="paragraph">
<p>In the following example, the compiler creates a file called pipeline.yaml, which contains a hermetic representation of your pipeline. The output is called an Intermediate Representation (IR) YAML, which is a serialized PipelineSpec protocol buffer message.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>  from kfp import compiler, dsl
  @dsl.component
  def comp(message: str) -&gt; str:
      print(message)
      return message
  @dsl.pipeline
  def my_pipeline(message: str) -&gt; str:
      """My ML pipeline."""
      return comp(message=message).output
  compiler.Compiler().compile(my_pipeline, package_path='pipeline.yaml')</pre>
</div>
</div>
<div class="paragraph">
<p>The contents of the file are not intended to be human-readable, however the comments at the top of the file provide a summary of the pipeline:</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ir_yaml"><a class="anchor" href="#_ir_yaml"></a>IR YAML</h3>
<div class="paragraph">
<p>The IR YAML is an intermediate representation of a compiled pipeline or component. It is an instance of the PipelineSpec protocol buffer message type, which is a platform-agnostic pipeline representation protocol. It is considered an intermediate representation because the KFP backend compiles PipelineSpec to Argo Workflow YAML as the final pipeline definition for execution.</p>
</div>
<div class="paragraph">
<p>Unlike the v1 component YAML, the IR YAML is not intended to be written directly. While IR YAML is not intended to be easily human-readable, you can still inspect it if you know a bit about its contents:</p>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_argo_workflows"><a class="anchor" href="#_what_is_argo_workflows"></a>What is Argo Workflows?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD.</p>
</div>
<div class="paragraph">
<p>Define workflows where each step in the workflow is a container.
Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG).
Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes.
Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_notes_of_using_kfp_sdk"><a class="anchor" href="#_notes_of_using_kfp_sdk"></a>Notes of using kfp SDK</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_pipeline_parameter_passing"><a class="anchor" href="#_pipeline_parameter_passing"></a>Pipeline Parameter Passing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.</p>
</div>
<div class="sect2">
<h3 id="_input_parameters"><a class="anchor" href="#_input_parameters"></a>Input Parameters</h3>
<div class="ulist">
<ul>
<li>
<p>Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.</p>
</li>
<li>
<p>Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_output_parameters"><a class="anchor" href="#_output_parameters"></a>Output Parameters</h3>
<div class="ulist">
<ul>
<li>
<p>Output values are returned via files.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_passing_parameters_via_files"><a class="anchor" href="#_passing_parameters_via_files"></a>Passing Parameters via Files</h3>
<div class="paragraph">
<p>To pass an input parameter as a file, the function argument needs to be annotated using the <em>InputPath</em> annotation.
For returning data from a step as a file, the function argument needs to be annotated using the <em>OutputPath</em> annotation.</p>
</div>
<div class="paragraph">
<p><strong>In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.</strong></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There are other parameter annotations available to handle specialised file types
such as <em>InputBinaryFile</em>, <em>OutputBinaryFile</em>.</p>
</div>
<div class="paragraph">
<p>The full annotation list is in the <a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html" target="_blank" rel="noopener">KFP component documentation</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_returning_multiple_values_from_a_task"><a class="anchor" href="#_returning_multiple_values_from_a_task"></a>Returning multiple values from a task</h3>
<div class="paragraph">
<p>If you return a single small value from your component using the <em>return</em> statement, the output parameter is named <strong><em>output</em></strong>.
It is, however, possible to return multiple small values using the Python <em>collection</em> library method <em>namedtuple</em>.</p>
</div>
<div class="paragraph">
<p>From a <a href="https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb" target="_blank" rel="noopener">Kubeflow pipelines example</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def produce_two_small_outputs() -&gt; NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])</code></pre>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If the argument name ends with <em>path and the argument is annotated as an _kfp.components.InputPath</em> or <em>kfp.components.OutputPath</em>, the parameter name is the argument name with the trailing _path removed.</p>
</li>
<li>
<p>If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.</p>
</li>
<li>
<p>If you return a single small value from your component using the return statement, the output parameter is named <strong>output</strong>.</p>
</li>
<li>
<p>If you return several small values from your component by returning a <em>collections.namedtuple</em>, the SDK uses the tuple’s field names as the output parameter names.</p>
</li>
<li>
<p>Otherwise, the SDK uses the argument name as the parameter name.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In the Argo Yaml definition you can see the definition of the <em>input and output artifacts</em>. This can be useful for debugging purposes.</p>
</div>
<div class="paragraph">
<p>You can also see the locations of data stored into the S3 bucket e.g. <em>artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz</em></p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_execution_on_openshift"><a class="anchor" href="#_execution_on_openshift"></a>Execution on OpenShift</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To enable the <em>pipeline</em> to run on OpenShift we need to pass it the associated <em>kubernetes</em> resources</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>volumes</em></p>
</li>
<li>
<p><em>environment variables</em></p>
</li>
<li>
<p><em>node selectors, taints and tolerations</em></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_volumes"><a class="anchor" href="#_volumes"></a>Volumes</h4>
<div class="paragraph">
<p>Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.</p>
</div>
<div class="paragraph">
<p>Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="paragraph">
<p>The volumes are mounted into the containers using the <strong><em>add_pvolumes</em></strong> method:</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
</div>
<div class="sect3">
<h4 id="_environment_variables"><a class="anchor" href="#_environment_variables"></a>Environment Variables</h4>
<div class="paragraph">
<p>Environment variables can be added to the pod using the <strong><em>add_env_variable</em></strong> method.</p>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <strong><em>env_from_secret</em></strong> utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the <em>AWS_ACCESS_KEY_ID</em> value is extracted from the <em>s3-secret</em> secret and added to the container defintion as the <em>s3_access_key</em> environment variable.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_node_selectors_taints_and_tolerations"><a class="anchor" href="#_node_selectors_taints_and_tolerations"></a>Node Selectors, Taints and Tolerations</h3>
<div class="paragraph">
<p>Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality.</p>
</div>
<div class="paragraph">
<p>In our example we&#8217;re using the nodes with an attached GPU to execute the step. To do this we need to:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the requisite toleration:</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add the <em>toleration</em> to the pod and add a <em>node selector</em> constraint.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$sample-pipeline-full.py[]</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You could also use this approach to ensure that pods without GPU needs are <strong>not</strong> scheduled to nodes with GPUs.</p>
</div>
<div class="paragraph">
<p>For global pipeline pod settings take a look at the <strong><em>PipelineConf</em></strong> class in the '<a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'" target="_blank" rel="noopener">KFP SDK Documentation</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We have only covered a <em>subset</em> of what&#8217;s possible with the <em>KFP SDK</em>.</p>
</div>
<div class="paragraph">
<p>It is also possible to customize significant parts of the <em>pod spec</em> definition with:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Init and Sidecar Pods</p>
</li>
<li>
<p>Pod affinity rules</p>
</li>
<li>
<p>Annotations and labels</p>
</li>
<li>
<p>Retries and Timeouts</p>
</li>
<li>
<p>Resource requests and limits</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>See the the <a href="https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html" target="_blank" rel="noopener">KFP SDK Documentation</a> for more details.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_pipeline_execution"><a class="anchor" href="#_pipeline_execution"></a>Pipeline Execution</h3>

</div>
<div class="sect2">
<h3 id="_submitting_a_pipeline_and_triggering_a_run"><a class="anchor" href="#_submitting_a_pipeline_and_triggering_a_run"></a>Submitting a Pipeline and Triggering a run</h3>
<div class="paragraph">
<p>The following code demonstrates how to submit and trigger a pipeline run from a <em>Red Hat OpenShift AI WorkBench</em>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if __name__ == '__main__':
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_externally_triggering_a_dsp_pipeline_run"><a class="anchor" href="#_externally_triggering_a_dsp_pipeline_run"></a>Externally Triggering a DSP pipeline run</h3>
<div class="paragraph">
<p>In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>File added to S3 bucket.</p>
</li>
<li>
<p>S3 triggers the send of a webhook payload to an <em>OCP Serverless</em> function.</p>
</li>
<li>
<p>The <em>Serverless</em> function parses the payload and invokes the configured <em>DSP pipeline</em>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>We&#8217;re not going to go through the code and configuration for this, but here is the code to trigger the pipeline.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$dsp_trigger.py[]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The full code is <a href="#attachment$dsp_trigger.py" class="xref unresolved">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <em>pipeline</em> needs to have already been submitted to the DSP runtime.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_data_handling_in_data_science_pipelines"><a class="anchor" href="#_data_handling_in_data_science_pipelines"></a>Data Handling in Data Science Pipelines</h2>
<div class="sectionbody">
<div class="paragraph">
<p>DSP have two sizes of data, conveniently named <strong><em>Small Data</em></strong> and <strong><em>Big Data</em></strong>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><em>Small Data</em> is considered anything that can be passed as a <em>command line argument</em> for example <em>Strings</em>, <em>URLS</em>, <em>Numbers</em>. The overall size should not exceed a few <em>kilobytes</em>.</p>
</li>
<li>
<p>Unsurprisingly, everything else is considered <em>Big Data</em> and should be passed as files.</p>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_handling_large_data_sets"><a class="anchor" href="#_handling_large_data_sets"></a>Handling large data sets</h3>
<div class="paragraph">
<p>DSP support two methods by which to pass large data sets aka <em>Big Data</em> between pipeline steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong><em>Argo Workspaces</em></strong>.</p>
</li>
<li>
<p><strong><em>Volume based data passing method</em></strong>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Data Science Projects <strong><em>Data Connection</em></strong> S3 storage is used to store <em>Output Artifacts</em> and <em>Parameters</em> of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_volume_based_data_passing_method"><a class="anchor" href="#_volume_based_data_passing_method"></a>Volume-based data passing method</h3>
<div class="paragraph">
<p>This approach uses a pre-created OpenShift storage volume (aka <em>PVC</em>) to pass data between the pipeline steps.
An example of this is in the <a href="https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py" target="_blank" rel="noopener">KFP compiler tests</a> which we will discuss here.</p>
</div>
<div class="paragraph">
<p>First create the volume to be used and assign it to a variable:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then add definition to the <em>pipeline configuration</em>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Unresolved include directive in modules/chapter4/pages/section1.adoc - include::example$artifact_passing_using_volume.py[]</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <strong><em>data-volume PVC claim</em></strong> needs to exist in the OpenShift namespace while running the pipeline, else the <em>pipeline execution pod</em> fails to deploy and the run terminates.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To pass big data using cloud provider volumes, it&#8217;s recommended to use the <strong><em>volume-based data passing method</em></strong>.</p>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
