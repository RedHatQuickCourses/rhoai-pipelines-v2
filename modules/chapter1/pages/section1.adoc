= RHOAI DSP specifics


== Specific Data Science Pipeline terminology in OpenShift AI DSP

 . *Pipeline* - is a workflow definition containing the steps and their input and output artifacts.

 . *Run* - is a single execution of a pipeline. A run can be a one off execution of a pipeline, or pipelines can be scheduled as a recurring run.

 . *Task* - is a self-contained pipeline component that represents an execution stage in the pipeline.

 . *Artifact* - Steps have the ability to create artifacts, which are objects that can be persisted after the execution of the step completes. Other steps may use those artifacts as inputs and some artifacts may be useful references after a pipeline run has completed. Artifacts automatically stored by Data Science Pipelines in S3 compatible storage.

 . *Experiment* - is a logical grouping of runs for the purpose of comparing different pipelines

 . *Execution* -  is an instance of a Task/Component



== Managing Data Science Pipelines 2.0 

=== Configuring a pipeline server

Before you can successfully create a pipeline in OpenShift AI, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.
 
 * You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.
 * You have created a data science project that you can add a pipeline server to.
 * If you are configuring a pipeline server with an external database
 ** Red Hat recommends that you use MySQL version 8.x.
 ** Red Hat recommends that you use at least MariaDB version 10.5.

=== Defining a  Pipeline 2.0 
Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to configure its execution settings.

You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about creating pipelines in JupyterLab, see Creating Pipelines in Elyra in the section below. For more information about the Elyra JupyterLab extension, https://elyra.readthedocs.io/en/v2.0.0/getting_started/overview.html[see Elyra Documentation.]


=== Importing a  Pipeline 2.0 
To help you begin working with data science pipelines in OpenShift AI, you can import a YAML file containing your pipelineâ€™s code to an active pipeline server, or you can import the YAML file from a URL.
This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.


=== Pipeline Actions

OpenShift AI data science pipelines supports the following actions:

 . Creating
 . Scheduling
 . Executing
 . Viewing
 . Archiving
 . Restoring
 . Deleting
 . Stopping
 . Duplicating

=== Working with pipeline logs
You can review and analyze step logs for each step in a triggered pipeline run.
To help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using the log viewer in the OpenShift AI dashboard. 

 * Viewing logs
 * Downloading logs


=== Technical Knowledge

OpenShift AI uses Kubeflow pipelines with Argo workflows as the engine. Kubeflow provides a rich set of tools for managing ML workloads, while Argo workflows offer powerful automation capabilities. Together, they enable us to create robust, scalable, and manageable pipelines for AI model development and serving.

Pipelines can include various components, such as data ingestion, data preprocessing, model training, evaluation, and deployment. These components can be configured to run in a specific order, and the pipeline can be executed multiple times to produce different versions of models or artifacts.

Additionally, pipelines can support control flows to handle complex dependencies between tasks. Once a pipeline is defined, executing it becomes a simple RUN command, and the status of each execution can be tracked and monitored, ensuring that the desired outputs are produced successfully.

In summary, data science pipelines are an essential tool for automating and managing the ML lifecycle, enabling data scientists to create end-to-end workflows, reduce human error, and ensure consistent, high-quality results. 

Let's explore how to build and deploy these powerful pipelines using OpenShift AI data science pipelines.

