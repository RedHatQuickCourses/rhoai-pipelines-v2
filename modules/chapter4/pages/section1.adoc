= blah blah blah 2


Overview
To submit a pipeline for execution, you must compile it to YAML with the KFP SDK compiler.

In the following example, the compiler creates a file called pipeline.yaml, which contains a hermetic representation of your pipeline. The output is called an Intermediate Representation (IR) YAML, which is a serialized PipelineSpec protocol buffer message.

----
  from kfp import compiler, dsl
  @dsl.component
  def comp(message: str) -> str:
      print(message)
      return message
  @dsl.pipeline
  def my_pipeline(message: str) -> str:
      """My ML pipeline."""
      return comp(message=message).output
  compiler.Compiler().compile(my_pipeline, package_path='pipeline.yaml')
----

The contents of the file are not intended to be human-readable, however the comments at the top of the file provide a summary of the pipeline:

=== IR YAML
The IR YAML is an intermediate representation of a compiled pipeline or component. It is an instance of the PipelineSpec protocol buffer message type, which is a platform-agnostic pipeline representation protocol. It is considered an intermediate representation because the KFP backend compiles PipelineSpec to Argo Workflow YAML as the final pipeline definition for execution.

Unlike the v1 component YAML, the IR YAML is not intended to be written directly. While IR YAML is not intended to be easily human-readable, you can still inspect it if you know a bit about its contents:


== What is Argo Workflows?
Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD.

Define workflows where each step in the workflow is a container.
Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG).
Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes.
Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.

== Notes of using kfp SDK

== Pipeline Parameter Passing
As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.

=== Input Parameters

* Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.
* Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.

=== Output Parameters

* Output values are returned via files.

=== Passing Parameters via Files
To pass an input parameter as a file, the function argument needs to be annotated using the _InputPath_ annotation.
For returning data from a step as a file, the function argument needs to be annotated using the _OutputPath_ annotation.

*In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.*

// For example, in our sample pipeline we use the _parameter_data_ argument of the _fraud-detection.yaml_ return multiples performance metrics data values as a file. Here's the function definition with the _OutputPath_ annotation

[TIP]
====
There are other parameter annotations available to handle specialised file types 
such as _InputBinaryFile_, _OutputBinaryFile_. 

The full annotation list is in the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html[KFP component documentation, window=_blank].

====

=== Returning multiple values from a task 
If you return a single small value from your component using the _return_ statement, the output parameter is named *_output_*.
It is, however, possible to return multiple small values using the Python _collection_ library method _namedtuple_.

From a https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb[Kubeflow pipelines example, window=_blank]
 
[source,python]
----
def produce_two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])
----

====
The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:

    . If the argument name ends with _path and the argument is annotated as an _kfp.components.InputPath_ or _kfp.components.OutputPath_, the parameter name is the argument name with the trailing _path removed.
    . If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.
    . If you return a single small value from your component using the return statement, the output parameter is named *output*.
    . If you return several small values from your component by returning a _collections.namedtuple_, the SDK uses the tuple’s field names as the output parameter names.

    . Otherwise, the SDK uses the argument name as the parameter name.
====

[TIP]
====
In the Argo Yaml definition you can see the definition of the _input and output artifacts_. This can be useful for debugging purposes.


You can also see the locations of data stored into the S3 bucket e.g. _artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz_
====

== Execution on OpenShift

To enable the _pipeline_ to run on OpenShift we need to pass it the associated _kubernetes_ resources 

* _volumes_ 
* _environment variables_
* _node selectors, taints and tolerations_

==== Volumes
Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.

Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.

include::example$sample-pipeline-full.py[lines=453..462]

The volumes are mounted into the containers using the *_add_pvolumes_* method:

include::example$sample-pipeline-full.py[lines=495..497]

==== Environment Variables

Environment variables can be added to the pod using the *_add_env_variable_* method. 

include::example$sample-pipeline-full.py[lines=471..475]

[NOTE]
====
The *_env_from_secret_* utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the _AWS_ACCESS_KEY_ID_ value is extracted from the _s3-secret_ secret and added to the container defintion as the _s3_access_key_ environment variable.
====

=== Node Selectors, Taints and Tolerations

Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality. 

In our example we're using the nodes with an attached GPU to execute the step. To do this we need to:


. Create the requisite toleration:

include::example$sample-pipeline-full.py[lines=464..467]

. Add the _toleration_ to the pod and add a _node selector_ constraint.

include::example$sample-pipeline-full.py[lines=477..480]


[TIP]
====
You could also use this approach to ensure that pods without GPU needs are *not* scheduled to nodes with GPUs.

For global pipeline pod settings take a look at the *_PipelineConf_* class in the 'https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'[KFP SDK Documentation, window=_blank]. 
====


[NOTE]
====
We have only covered a _subset_ of what's possible with the _KFP SDK_.

It is also possible to customize significant parts of the _pod spec_ definition with:

* Init and Sidecar Pods
* Pod affinity rules
* Annotations and labels
* Retries and Timeouts
* Resource requests and limits

See the the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html[KFP SDK Documentation, window=_blank] for more details.
====



=== Pipeline Execution

=== Submitting a Pipeline and Triggering a run

The following code demonstrates how to submit and trigger a pipeline run from a _Red Hat OpenShift AI WorkBench_.

[source, python]
if __name__ == '__main__':  
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )

=== Externally Triggering a DSP pipeline run

In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:

. File added to S3 bucket.
. S3 triggers the send of a webhook payload to an _OCP Serverless_ function.
. The _Serverless_ function parses the payload and invokes the configured _DSP pipeline_.

We're not going to go through the code and configuration for this, but here is the code to trigger the pipeline.

[source,python]
include::example$dsp_trigger.py[lines=34..51]


The full code is xref:attachment$dsp_trigger.py[here].

[NOTE]
====
The _pipeline_ needs to have already been submitted to the DSP runtime.
====


== Data Handling in Data Science Pipelines
DSP have two sizes of data, conveniently named *_Small Data_* and *_Big Data_*.

. _Small Data_ is considered anything that can be passed as a _command line argument_ for example _Strings_, _URLS_, _Numbers_. The overall size should not exceed a few _kilobytes_.

. Unsurprisingly, everything else is considered _Big Data_ and should be passed as files.

=== Handling large data sets

DSP support two methods by which to pass large data sets aka _Big Data_ between pipeline steps:

. *_Argo Workspaces_*.
. *_Volume based data passing method_*.

[NOTE]
====
The Data Science Projects *_Data Connection_* S3 storage is used to store _Output Artifacts_ and _Parameters_ of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.
====



=== Volume-based data passing method
This approach uses a pre-created OpenShift storage volume (aka _PVC_) to pass data between the pipeline steps.
An example of this is in the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py[KFP compiler tests, window=_blank] which we will discuss here.

First create the volume to be used and assign it to a variable:
[source,python]
include::example$artifact_passing_using_volume.py[lines=78..79]

[source,python]
include::example$artifact_passing_using_volume.py[lines=81..88]

Then add definition to the _pipeline configuration_:
[source,python]
include::example$artifact_passing_using_volume.py[lines=91..93]


[IMPORTANT]
====
The *_data-volume PVC claim_* needs to exist in the OpenShift namespace while running the pipeline, else the _pipeline execution pod_ fails to deploy and the run terminates.
====

To pass big data using cloud provider volumes, it's recommended to use the *_volume-based data passing method_*.


