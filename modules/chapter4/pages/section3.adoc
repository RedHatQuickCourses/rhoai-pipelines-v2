This is just reference info -tb updated

=== Data Science Pipelines

[cols="1,1,1,1"]
|===
|OpenShift AI Resource Name | Kubernetes Resource Name | Custom Resource | Description 

|Data Science Pipeline Application
|datasciencepipelinesapplications.datasciencepipelinesapplications.opendatahub.io
|Yes
|DSPA's create an instance of Data Science Pipelines.  DSPA's require a data connection and an S3 bucket to create the instance.  DSPA's are namespace scoped to prevent leaking data across multiple projects.

|Pipelines
|N/A
|N/A
|When developing a pipeline, depending on the tool, users may generate a YAML based PipelineRun object that is then uploaded into the Dashboard to create an executable pipeline.  Even though this yaml object is a valid Tekton PipelineRun it is intended to be uploaded to the Dashboard, and not applied directly to the cluster.

|Pipeline Runs
|pipelineruns.tekton.dev
|Yes
|A pipeline can be executed in a number of different ways, including from the Dashboard, which will result in the creation of a pipelinerun.

|===


* Create a local *virtualenv* on your workstation, install the *kfp-tekton* package
+
[source,bash]
----
$ mkdir kfp #create a folder anywhere
$ cd kfp
$ python3 -m venv .venv
$ source .venv/bin/activate # activate the venv
$(.venv) pip install kfp-tekton~=1.5.0
----
+
IMPORTANT: Using the correct Python module versions is critical to avoid conflicts between the KFP SDK and Data Science Pipelines versions. Install the latest *kfp-tekton* module version 1.5.x. Installing the 1.8.x versions will result in failures during pipeline runs.

=== Building and deploying a Pipeline

. Download the xref:attachment$coin-toss.py[Coin Toss Pipeline] Python file and copy it to your *kfp* folder where you created the virtualenv. Inspect the file to understand how  the pipeline is composed using plain Python functions. The pipeline name and other metadata is provided using the *@dsl.pipeline* annotation. Note the invocation to the *TektonCompiler* in the `main` function:
+
[source,python]
----
...
from kfp import dsl
from kfp import components
...

flip_coin_op = components.create_component_from_func(
    flip_coin, base_image='python:alpine3.6')
print_op = components.create_component_from_func(
    print_msg, base_image='python:alpine3.6')
random_num_op = components.create_component_from_func(
    random_num, base_image='python:alpine3.6')

@dsl.pipeline(
    name='conditional-execution-pipeline',
    description='Shows how to use dsl.Condition().'
)
def flipcoin_pipeline():
    flip = flip_coin_op()
    with dsl.Condition(flip.output == 'heads'):
        random_num_head = random_num_op(0, 9)
        with dsl.Condition(random_num_head.output > 5):
            print_op('heads and %s > 5!' % random_num_head.output)
        with dsl.Condition(random_num_head.output <= 5):
            print_op('heads and %s <= 5!' % random_num_head.output)

    with dsl.Condition(flip.output == 'tails'):
        random_num_tail = random_num_op(10, 19)
        with dsl.Condition(random_num_tail.output > 15):
            print_op('tails and %s > 15!' % random_num_tail.output)
        with dsl.Condition(random_num_tail.output <= 15):
            print_op('tails and %s <= 15!' % random_num_tail.output)


if __name__ == '__main__':
    from kfp_tekton.compiler import TektonCompiler
    TektonCompiler().compile(flipcoin_pipeline, __file__.replace('.py', '.yaml'))
----

. Compile the Python file into a Tekton resource definition. Run the following command from within the virtualenv.
+
[source,python]
----
$(.venv) python3 coin-toss.py
----

. A YAML file called `coin-toss.yaml` containing a Tekton *_PipelineRun_* resource will be created. Inspect this file to understand how the `kfp-tekton` SDK has transformed your pipeline definition in Python into a runnable Tekton `PipelineRun` resource:
+
[source,yaml]
----
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: coin-toss-pipeline
  annotations:
    tekton.dev/output_artifacts
    ...
    tekton.dev/input_artifacts:
    ...
----

. The resulting yaml file *coin-toss.yaml* can then be uploaded through the RHOAI web console. Navigate to the `pipelines-example` DS project that you created in the previous section on Elyra piplines. Under the `Pipelines` section, click on `Import Pipeline`:
+
image::import-pipeline.png[title=Import Tekton YAML Resource File]

. Enter *coin-toss-pipeline* in the `Pipeline name` field, provide a brief description and upload the `coin-toss.yaml` file. Click `Import pipeline` to import the pipeline.

== Real World Example

WARNING: This entire section is to cover some theoretical concepts around creating Kubeflow pipelines using the KFP SDK. You are *NOT* expected to run this in your set up since the system has numerous complex components which are missing in your runtime environment. Do *NOT* execute any code or commands in this section. This section is for informational and pedagogical purposes only!

In this section we're going to demonstrate a real world Data Science Pipelines scenario. 

****
In this scenario we have a remote edge device which uses an AI model to manage the characteristics of its battery usage depending on the environment it's deployed in. On a regular schedule it uploads battery events via a data gateway, and those battery events are used to train a model which is then retrieved by the device and used. 

image::openshift-ai-dsp-edge.png[title=AI/ML at the edge example]
****

The entire pipeline definition is available xref:attachment$sample-pipeline-full.py[here, window=_blank] for inspection. We're not going to go through all of it but focus on the key aspects of it. 

The actual pipeline is defined by the following function:

****
[source,python]
include::example$sample-pipeline-full.py[lines=445..454]
****

The *@dsl.pipeline* parameters provide the name and description if you were uploading the pipeline via an API call. The DSP UI can overwrite these values.

The function *_edgetest_pipeline_* function is the implementation of the pipeline.

=== Pipeline Parameters
The pipeline has four parameters:

* _file_obj_ and _src_bucket_ refer to S3 bucket details and can be ignored.
* _VIN_ is the edge device identifier and has a default value of 412356.
* _epoch_count_ is the number of training epochs to be used.

In the _Create Run_ UI these parameters are available so that users can override the values as they need.

****
image::pipeline-parameters.png[]
****

=== Pipeline Steps
The file contains the following Python functions, which roughly correspond to the steps in the diagram above

* load_trigger_data()
* prep_data_train_model()
* model_upload_notify()
* model_inference()

These functions are mapped into individual containers by using the _create_component_from_func_ function. You can specify the container _base_image_ to use as well as any additional Python packages to be installed into the container at execution time.


The Python functions can be used in multiple different step definitions; in the example the _prep_data_train_model_ function is used in the _prep_data_train_op_ and the _prep_inference_data_op_ containers.

The pipeline execution _graph_ is created using the following code:



[IMPORTANT] 
====
The execution order of the graph is top down but also can be controlled by using the *_.after()_* operator. 

There are other operators which control the flow of pipeline execution such as _Condition_ , _ExitHandler_, _ParallelFor_ .
These are not covered as part of this course but the https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/DSL%20-%20Control%20structures/DSL%20-%20Control%20structures.py[KFP documentation, window=_blank] has examples.
====

The following diagram shows the order of execution.

****
image::pipeline-graph.png[]
****
