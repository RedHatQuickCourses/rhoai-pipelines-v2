= Kubeflow Pipelines SDK

In the previous section, we created a two task pipeline by using the Elyra GUI pipeline editor. The kfp SDK provides a Python API for creating pipelines. The SDK is available as a Python package that you can install by using the `pip install kfp` command. With this package, you can use Python code to create a pipeline and then compile it to YAML format. Then we can import the YAML code into OpenShift AI.

This course does not delve into the details of how to use the SDK. Instead, it provides the files for you to view and upload.



== Lab Exercise: Importing a Data Science Pipeline

=== Prerequisites 

* Continue to use the `fraud-detection` Data Science Project that you created in the previous section. We won't need a workbench in this section, but you should have completed all lab exercises up through Data Science Pipelines / OpenShift AI Resources section of the course in order for the environment to support pipeline creation.

image::import_pipeline_yaml.gif[width=600]


. In the RHOAI side navigation menu, click `Data Science Pipelines

. Select the import pipeline button

. For the pipeline name use: fraud-detection-example

. Select link below to download the fraud-detection.yaml file to import.
+
https://github.com/RedHatQuickCourses/rhoai-pipelines-v2/blob/main/downloads/fraud_detection.yaml[Fraud Detection Pipeline Definition Yaml, window=blank]
+
image::download_pipeline_yaml.png[width=600]
+
. Select the 'upload a file' Pipeline radial button.

. Click on upload.

. Browse to the folder where you downloaded the fraud-detection.yaml file, then click Import pipeline.

.. Optionally, it is possible to import the pipeline from a url.

Once the pipeline definition is imported, it can be viewed in the data science pipeline dashboard.  

There are three tabs in DSP dashboard:

 * Graph - the DAG view of the pipeline defined
 * Summary - shows the pipeline spec and version IDs
 * Pipeline spec - is the import yaml file that defined the pipeline.

The pipeline is now available to be executed, but currently there have been no *one-off or scheduled runs* for this pipeline, it has only been defined in the system. 





//In the Minio web console, click `Object Browser > data-science-pipelines > artifacts > PIPELINE_NAME-XXX`, where `xxxxx` is a randomly generated number for the pipeline run. You should the output artifacts generated by the pipeline.

// image::object-store-after-run.png[]

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize pipeline runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the Data Science Pipeline UI or programmatically using the _KFP SDK_.

image::create_pipeline_run.gif[]

To create a _run_ for the fraud-detection-example pipeline we just imported.

 . Locate the fraud-detection-example in the data science pipeline menu / dashboard.

 . Use the menu at the far right to:

  .. upload a new version of the pipeline
  .. create a schedule run for sometime in the future
  .. create a one-off run

 . Select the option to create a new run.

To execute the run, we need to imput some information:

 . Run Type: Scheduled runs are exectued from different dashboard - skip this step

 . Define the project and experiment name:

 .. The project name is immutable and cannot be changed at this time.

 .. For the experiment, choose an existing experiment from the list or create a new one.  Select create a new experiment. 

 . Run Details:  
 
 .. Specify a name for this run

 .. Add a description

The final section describes the pipeline 

 . Select the pipeline to Run from the drop down

 . Select the version of the pipeline if available.

Depending on Pipeline Definition, some parameters must be specified at runtime.

In this case there are two required parameters, which allow this pipeline to have different inputs.

 . The first parameter is a url location of the data file to be imported during the Run.

 . The second paramenter is number of epochs.
 +
 [NOTE]
This epoch's number is an important hyperparameter for the algorithm. It specifies the number of epochs or complete passes of the entire training dataset passing through the training or learning process of the algorithm.

Select _Create_ Run to start the run.

------
------
