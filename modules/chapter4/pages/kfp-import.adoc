= Kubeflow Pipelines SDK

In the previous section, we created a two task pipeline by using the Elyra GUI pipeline editor. The kfp SDK provides a Python API for creating pipelines. The SDK is available as a Python package that you can install by using the `pip install kfp` command. With this package, you can use Python code to create a pipeline and then compile it to YAML format. Then we can import the YAML code into OpenShift AI.

This course does not delve into the details of how to use the SDK. Instead, it provides the files for you to view and upload.



== Lab Exercise: Importing a Data Science Pipeline

=== Prerequisites 

* Continue to use the `fraud-detection` Data Science Project that you created in the previous section. We won't need a workbench in this section, but you should have completed all lab exercises up through Data Science Pipelines / OpenShift AI Resources section of the course in order for the environment to support pipeline creation.

image::import_pipeline_yaml.gif[width=600]


. In the RHOAI side navigation menu, click `Data Science Pipelines

. Select the import pipeline button

. For the pipeline name use: fraud-detection-example

. Select the import by url option

+
```
https://github.com/RedHatQuickCourses/rhoai-pipelines-v2/blob/main/downloads/fraud_detection.yaml
```
. Select the Import Pipeline button at the bottom.

Once the pipeline definition is imported, it can be viewed in the data science pipeline dashboard.  

There are three tabs in DSP dashboard:

 * Graph - the DAG view of the pipeline defined
 * Summary - shows the pipeline spec and version IDs
 * Pipeline spec - is the import yaml file that defined the pipeline.

The pipeline is now available to be executed, but currently there have been no *one-off or scheduled runs* for this pipeline, it has only been defined in the system. 





//In the Minio web console, click `Object Browser > data-science-pipelines > artifacts > PIPELINE_NAME-XXX`, where `xxxxx` is a randomly generated number for the pipeline run. You should the output artifacts generated by the pipeline.

// image::object-store-after-run.png[]

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize pipeline runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the Data Science Pipeline UI or programmatically using the _KFP SDK_.

image::create_pipeline_run.gif[]

To create a _run_ for the fraud-detection-example pipeline we just imported.

 . Locate the fraud-detection-example in the data science pipeline menu / dashboard.

 . Use the menu at the far right to:

  .. upload a new version of the pipeline
  .. create a schedule run for sometime in the future
  .. create a one-off run

 . Select the option to create a new run.

To execute the run, we need to imput some information:

 . Run Type: Scheduled runs are exectued from different dashboard - skip this step

 . Define the project and experiment name:

 .. The project name is immutable and cannot be changed at this time.

 .. For the experiment, choose an existing experiment from the list or create a new one.  Select create a new experiment. 

 . Run Details:  
 
 .. Specify a name for this run

 .. Add a description

The final section describes the pipeline 

 . Select the pipeline to Run from the drop down

 . Select the version of the pipeline if available.

Depending on Pipeline Definition, some parameters must be specified at runtime.

In this case there are two required parameters, which allow this pipeline to have different inputs.

 . The first parameter is a url location of the data file to be imported during the Run.

 . The second paramenter is number of epochs.
 +
 [NOTE]
This epoch's number is an important hyperparameter for the algorithm. It specifies the number of epochs or complete passes of the entire training dataset passing through the training or learning process of the algorithm.

Select _Create_ Run to start the run.

------
------


=== Pipeline Parameter Passing
As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.

==== Input Parameters

* Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.
* Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.

==== Output Parameters

* Output values are returned via files.

==== Passing Parameters via Files
To pass an input parameter as a file, the function argument needs to be annotated using the _InputPath_ annotation.
For returning data from a step as a file, the function argument needs to be annotated using the _OutputPath_ annotation.

*In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.*

// For example, in our sample pipeline we use the _parameter_data_ argument of the _fraud-detection.yaml_ return multiples performance metrics data values as a file. Here's the function definition with the _OutputPath_ annotation

[TIP]
====
There are other parameter annotations available to handle specialised file types 
such as _InputBinaryFile_, _OutputBinaryFile_. 

The full annotation list is in the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html[KFP component documentation, window=_blank].

====

==== Returning multiple values from a task 
If you return a single small value from your component using the _return_ statement, the output parameter is named *_output_*.
It is, however, possible to return multiple small values using the Python _collection_ library method _namedtuple_.

From a https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb[Kubeflow pipelines example, window=_blank]
 
[source,python]
----
def produce_two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])
----

====
The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:

    . If the argument name ends with _path and the argument is annotated as an _kfp.components.InputPath_ or _kfp.components.OutputPath_, the parameter name is the argument name with the trailing _path removed.
    . If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.
    . If you return a single small value from your component using the return statement, the output parameter is named *output*.
    . If you return several small values from your component by returning a _collections.namedtuple_, the SDK uses the tuple’s field names as the output parameter names.

    . Otherwise, the SDK uses the argument name as the parameter name.
====

[TIP]
====
In the Argo Yaml definition you can see the definition of the _input and output artifacts_. This can be useful for debugging purposes.


You can also see the locations of data stored into the S3 bucket e.g. _artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz_
====

=== Execution on OpenShift

To enable the _pipeline_ to run on OpenShift we need to pass it the associated _kubernetes_ resources 

* _volumes_ 
* _environment variables_
* _node selectors, taints and tolerations_

==== Volumes
Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.

Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.

include::example$sample-pipeline-full.py[lines=453..462]

The volumes are mounted into the containers using the *_add_pvolumes_* method:

include::example$sample-pipeline-full.py[lines=495..497]

==== Environment Variables

Environment variables can be added to the pod using the *_add_env_variable_* method. 

include::example$sample-pipeline-full.py[lines=471..475]

[NOTE]
====
The *_env_from_secret_* utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the _AWS_ACCESS_KEY_ID_ value is extracted from the _s3-secret_ secret and added to the container defintion as the _s3_access_key_ environment variable.
====

==== Node Selectors, Taints and Tolerations

Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality. 

In our example we're using the nodes with an attached GPU to execute the step. To do this we need to:


. Create the requisite toleration:

include::example$sample-pipeline-full.py[lines=464..467]

. Add the _toleration_ to the pod and add a _node selector_ constraint.

include::example$sample-pipeline-full.py[lines=477..480]


[TIP]
====
You could also use this approach to ensure that pods without GPU needs are *not* scheduled to nodes with GPUs.

For global pipeline pod settings take a look at the *_PipelineConf_* class in the 'https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'[KFP SDK Documentation, window=_blank]. 
====


[NOTE]
====
We have only covered a _subset_ of what's possible with the _KFP SDK_.

It is also possible to customize significant parts of the _pod spec_ definition with:

* Init and Sidecar Pods
* Pod affinity rules
* Annotations and labels
* Retries and Timeouts
* Resource requests and limits

See the the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html[KFP SDK Documentation, window=_blank] for more details.
====



=== Pipeline Execution

==== Submitting a Pipeline and Triggering a run

The following code demonstrates how to submit and trigger a pipeline run from a _Red Hat OpenShift AI WorkBench_.

[source, python]
if __name__ == '__main__':  
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )

==== Externally Triggering a DSP pipeline run

In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:

. File added to S3 bucket.
. S3 triggers the send of a webhook payload to an _OCP Serverless_ function.
. The _Serverless_ function parses the payload and invokes the configured _DSP pipeline_.

We're not going to go through the code and configuration for this, but here is the code to trigger the pipeline.

[source,python]
include::example$dsp_trigger.py[lines=34..51]


The full code is xref:attachment$dsp_trigger.py[here].

[NOTE]
====
The _pipeline_ needs to have already been submitted to the DSP runtime.
====


== Data Handling in Data Science Pipelines
DSP have two sizes of data, conveniently named *_Small Data_* and *_Big Data_*.

. _Small Data_ is considered anything that can be passed as a _command line argument_ for example _Strings_, _URLS_, _Numbers_. The overall size should not exceed a few _kilobytes_.

. Unsurprisingly, everything else is considered _Big Data_ and should be passed as files.

=== Handling large data sets

DSP support two methods by which to pass large data sets aka _Big Data_ between pipeline steps:

. *_Argo Workspaces_*.
. *_Volume based data passing method_*.

[NOTE]
====
The Data Science Projects *_Data Connection_* S3 storage is used to store _Output Artifacts_ and _Parameters_ of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.
====



=== Volume-based data passing method
This approach uses a pre-created OpenShift storage volume (aka _PVC_) to pass data between the pipeline steps.
An example of this is in the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py[KFP compiler tests, window=_blank] which we will discuss here.

First create the volume to be used and assign it to a variable:
[source,python]
include::example$artifact_passing_using_volume.py[lines=78..79]

[source,python]
include::example$artifact_passing_using_volume.py[lines=81..88]

Then add definition to the _pipeline configuration_:
[source,python]
include::example$artifact_passing_using_volume.py[lines=91..93]


[IMPORTANT]
====
The *_data-volume PVC claim_* needs to exist in the OpenShift namespace while running the pipeline, else the _pipeline execution pod_ fails to deploy and the run terminates.
====

To pass big data using cloud provider volumes, it's recommended to use the *_volume-based data passing method_*.

