= Kubeflow Pipelines SDK

Red Hat OpenShift AI offers two out-of-the-box mechanisms to work with Data Science Pipelines in terms of building and running machine learning pipelines.

The first mechanism is the *Elyra Pipelines* JupyterLab extension, which provides a visual editor for creating pipelines based on Jupyter notebooks as well as `Python` or `R` scripts. 

The second mechanism, and the one discussed here is based on the *Kubeflow Pipelines SDK*. With the SDK, pipelines are built using `Python` scripts and submitted to the Data Science Pipelines runtime to be scheduled for execution.

While the Elyra extension offers an easy to use visual editor to compose pipelines, and is generally used for simple workflows, the Kubeflow Pipelines SDK (*kfp*) offers a flexible Python Domain Specific Language (DSL) API to create pipelines from Python code. This approach offers you flexibility in composing complex workflows and has the added benefit of offering all the Python tooling, frameworks and developer experience that comes with writing Python code.

OpenShift AI uses the *_Argo Wotkflows_* runtime to execute pipelines, which is why your Kubeflow pipeline containing Python code needs to be compiled into a yaml definition before it can be submitted to the runtime. Steps in the pipeline are executed as ephemeral pods (one per step).

[NOTE]
====
Data Science Pipelines in Red Hat OpenShift AI are managed by the *data-science-pipelines-operator-controller-manager* operator in the *redhat-ods-applications* namespace. The Custom Resource (CR) is an instance of *datasciencepipelinesapplications.datasciencepipelinesapplications.opendatahub.io*. 
====

== Exercise: Creating a Simple Data Science Pipeline with the KFP SDK

=== Prerequisites 

* Continue to use the `fraud-detection` Data Science Project that you created in the previous section. Ensure you complete the exercises in the previous section on Elyra. You will reuse several components from the previous exercise.



. In the RHOAI side navigation menu, click `Data Science Pipelines > Pipelines`It's possible to click on the graph nodes to reveal information of the steps.
+
// image::post-pipeline-run.png[title=Pipeline execution graphical view]

. Once the pipeline has completed, it is possible to access the output and pipeline artifacts (if used) in the object storage browser of the Minio web console. Open the Minio web console (you installed and configured Minio in the previous exercise on Elyra pipelines).
+
In the Minio web console, click `Object Browser > data-science-pipelines > artifacts > PIPELINE_NAME-XXX`, where `xxxxx` is a randomly generated number for the pipeline run. You should the output artifacts generated by the pipeline.
+
// image::object-store-after-run.png[]

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the DSP UI or programmatically using the _KFP SDK_.

//image::dsp-runs.png[]

NOTE: Experiments are part of the KFP SDK and are not currently covered in this course.


This is also visible in the Red Hat OpenShift AI user interface:

****
image::oai-pipeline-graph.png[]
****

=== Pipeline Parameter Passing
As each step of our pipeline is executed in an independent container, input parameters and output values are handled as follows.

==== Input Parameters

* Simple parameters - booleans, numbers, strings - are passed by value into the container as command line arguments.
* Complex types or large amounts of data are passed via files. The value of the input parameter is the file path.

==== Output Parameters

* Output values are returned via files.

==== Passing Parameters via Files
To pass an input parameter as a file, the function argument needs to be annotated using the _InputPath_ annotation.
For returning data from a step as a file, the function argument needs to be annotated using the _OutputPath_ annotation.

In both cases the actual value of the parameter is the file path and not the actual data. So the pipeline will have to read/write to the file as necessary.

For example, in our sample pipeline we use the _parameter_data_ argument of the _prep_data_train_model_ function to return multiple data values as a file. Here's the function definition with the _OutputPath_ annotation

[source,python]
include::example$sample-pipeline-full.py[lines=51]

Here's the actual writing of the data to the file

[source,python]
include::example$sample-pipeline-full.py[lines=238..242]

This data is then consumed in the _model_upload_notify_ function, passed via the _paramater_data_ with the _InputPath_ annotation.




[TIP]
====
There are other parameter annotations available to handle specialised file types 
such as _InputBinaryFile_, _OutputBinaryFile_. 

The full annotation list is in the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.components.html[KFP component documentation, window=_blank].

====

==== Returning multiple values from a step 
If you return a single small value from your component using the _return_ statement, the output parameter is named *_output_*.
It is, however, possible to return multiple small values using the Python _collection_ library method _namedtuple_.

From a https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb[Kubeflow pipelines example, window=_blank]
 
[source,python]
----
def produce_two_small_outputs() -> NamedTuple('Outputs', [('text', str), ('number', int)]):
    return ("data 1", 42)
consume_task3 = consume_two_arguments(produce2_task.outputs['text'], produce2_task.outputs['number'])
----

====
The KFP SDK uses the following rules to define the input and output parameter names in your component’s interface:

    . If the argument name ends with _path and the argument is annotated as an _kfp.components.InputPath_ or _kfp.components.OutputPath_, the parameter name is the argument name with the trailing _path removed.
    . If the argument name ends with _file, the parameter name is the argument name with the trailing _file removed.
    . If you return a single small value from your component using the return statement, the output parameter is named *output*.
    . If you return several small values from your component by returning a _collections.namedtuple_, the SDK uses the tuple’s field names as the output parameter names.

    . Otherwise, the SDK uses the argument name as the parameter name.
====

[TIP]
====
In the Argo Yaml definition you can see the definition of the _input and output artifacts_. This can be useful for debugging purposes.


You can also see the locations of data stored into the S3 bucket e.g. _artifacts/$PIPELINERUN/prep-data-train-model-2/parameter_data.tgz_
====

=== Execution on OpenShift

To enable the _pipeline_ to run on OpenShift we need to pass it the associated _kubernetes_ resources 

* _volumes_ 
* _environment variables_
* _node selectors, taints and tolerations_

==== Volumes
Our pipeline requires a number of volumes to be created and mounted into the executing pods. The volumes are primarily used for storage and secrets handling but can also be used for passing configuration files into the pods.

Before mounting the volumes into the pods they need to be created. The following code creates two volumes, one from a pre-existing PVC and another from a pre-existing secret.

include::example$sample-pipeline-full.py[lines=453..462]

The volumes are mounted into the containers using the *_add_pvolumes_* method:

include::example$sample-pipeline-full.py[lines=495..497]

==== Environment Variables

Environment variables can be added to the pod using the *_add_env_variable_* method. 

include::example$sample-pipeline-full.py[lines=471..475]

[NOTE]
====
The *_env_from_secret_* utility method also enables extracting values from secrets and mounting them as environment variables. In the example above the _AWS_ACCESS_KEY_ID_ value is extracted from the _s3-secret_ secret and added to the container defintion as the _s3_access_key_ environment variable.
====

==== Node Selectors, Taints and Tolerations

Selecting the correct worker node to execute a pipeline step is an important part of pipeline development. Specific nodes may have dedicated hardware such as GPUs; or there may be other constraints such as data locality. 

In our example we're using the nodes with an attached GPU to execute the step. To do this we need to:


. Create the requisite toleration:

include::example$sample-pipeline-full.py[lines=464..467]

. Add the _toleration_ to the pod and add a _node selector_ constraint.

include::example$sample-pipeline-full.py[lines=477..480]


[TIP]
====
You could also use this approach to ensure that pods without GPU needs are *not* scheduled to nodes with GPUs.

For global pipeline pod settings take a look at the *_PipelineConf_* class in the 'https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html?highlight=add_env_variable#kfp.dsl.PipelineConf'[KFP SDK Documentation, window=_blank]. 
====


[NOTE]
====
We have only covered a _subset_ of what's possible with the _KFP SDK_.

It is also possible to customize significant parts of the _pod spec_ definition with:

* Init and Sidecar Pods
* Pod affinity rules
* Annotations and labels
* Retries and Timeouts
* Resource requests and limits

See the the https://kubeflow-pipelines.readthedocs.io/en/1.8.22/source/kfp.dsl.html[KFP SDK Documentation, window=_blank] for more details.
====

=== Compiling to Tekton 

As stated previously, DSP Python scripts need to be compiled into Tekton definitions for execution. 
This can be achieved in multiple ways:

. Explicitly calling the "dsl-compile" command from the _kfp_ Python package giving the input and output files, then uploading the resultant yaml file to the DSP server via the UI.
. Adding the compile step to the Python script and then uploading the resultant yaml file to the DSP server via the UI.
. Adding the compile step to the Python script and uploading the resulting Tekton definition via an api call.

In our example we've chosen the second option:

include::example$sample-pipeline-full.py[lines=511]


The Tekton compiler also has a number of _global settings_, which are not covered here, see https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/kfp_tekton/compiler/pipeline_utils.py[here, window=_blank] for more details.


[IMPORTANT]
====
We have only covered a subset of the functionality available in DSP as it pertains to our real-life scenario. Please see the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/FEATURES.md[kfp-tekton features, window=_blank] document for more advanced functionality. The https://github.com/kubeflow/kfp-tekton/blob/master/guides/advanced_user_guide.md[KFP Tekton Advanced User Guide, window=_blank] also has more information.
====

=== Pipeline Execution

==== Submitting a Pipeline and Triggering a run

The following code demonstrates how to submit and trigger a pipeline run from a _Red Hat OpenShift AI WorkBench_.

[source, python]
if __name__ == '__main__':  
    kubeflow_endpoint = 'http://ds-pipeline-pipelines-definition:8888'
    sa_token_file_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    with open(sa_token_file_path, 'r') as token_file:
        bearer_token = token_file.read()
    print(f'Connecting to Data Science Pipelines: {kubeflow_endpoint}')
    client = TektonClient(
        host=kubeflow_endpoint,
        existing_token=bearer_token
    )
    result = client.create_run_from_pipeline_func(
        offline_scoring_pipeline,
        arguments={},
        experiment_name='offline-scoring-kfp'
    )

==== Externally Triggering a DSP pipeline run

In our real-world example above the entire pipeline is executed when a file is added to an S3 bucket. Here is the process followed:

. File added to S3 bucket.
. S3 triggers the send of a webhook payload to an _OCP Serverless_ function.
. The _Serverless_ function parses the payload and invokes the configured _DSP pipeline_.

We're not going to go through the code and configuration for this, but here is the code to trigger the pipeline.

[source,python]
include::example$dsp_trigger.py[lines=34..51]


The full code is xref:attachment$dsp_trigger.py[here].

[NOTE]
====
The _pipeline_ needs to have already been submitted to the DSP runtime.
====


== Data Handling in Data Science Pipelines
DSP have two sizes of data, conveniently named *_Small Data_* and *_Big Data_*.

. _Small Data_ is considered anything that can be passed as a _command line argument_ for example _Strings_, _URLS_, _Numbers_. The overall size should not exceed a few _kilobytes_.

. Unsurprisingly, everything else is considered _Big Data_ and should be passed as files.

=== Handling large data sets

DSP support two methods by which to pass large data sets aka _Big Data_ between pipeline steps:

. *_Tekton Workspaces_*.
. *_Volume based data passing method_*.

[NOTE]
====
The Data Science Projects *_Data Connection_* S3 storage is used to store _Output Artifacts_ and _Parameters_ of the stages of a pipeline. It is not intended to be used to pass large amounts of data between pipeline steps.
====

=== Tekton Workspaces

This uses the native Tekton mechanism for storing and passing data between stages of a _Tekton Pipeline_.
Tekton creates a *Workspace* to share large data files among tasks that run in the same pipeline. These _Workspaces_ are backed by dynamically created storage volumes which are removed once the pipeline has completed.

By default the storage is set to *_ReadWriteMany_*, the size is set to *_2Gi_* and uses the storage class called *_kfp-csi-s3_*. 

However, this can be changed to suit the target environment needs by setting the following *_Environment Variables_*:

. _DEFAULT_ACCESSMODES_
. _DEFAULT_STORAGE_SIZE_
. _DEFAULT_STORAGE_CLASS_

An example of this in the _Sample Pipeline_ is shown below:  

[source,python]
include::example$sample-pipeline-full.py[lines=503..505]

=== Volume-based data passing method
This approach uses a pre-created OpenShift storage volume (aka _PVC_) to pass data between the pipeline steps.
An example of this is in the https://github.com/kubeflow/kfp-tekton/blob/master/sdk/python/tests/compiler/testdata/artifact_passing_using_volume.py[KFP compiler tests, window=_blank] which we will discuss here.

First create the volume to be used and assign it to a variable:
[source,python]
include::example$artifact_passing_using_volume.py[lines=78..79]

[source,python]
include::example$artifact_passing_using_volume.py[lines=81..88]

Then add definition to the _pipeline configuration_:
[source,python]
include::example$artifact_passing_using_volume.py[lines=91..93]


[IMPORTANT]
====
The *_data-volume PVC claim_* needs to exist in the OpenShift namespace while running the pipeline, else the _pipeline execution pod_ fails to deploy and the run terminates.
====

To pass big data using cloud provider volumes, it's recommended to use the *_volume-based data passing method_*.

// == Exercise: Data Science Pipeline for Fraud Detection Scenario

// === Prerequisites 

// * Continue to use the `pipelines-example` Data Science Project that you created in the previous section. Ensure you complete the exercises in the previous section on Elyra. You will reuse several components from the previous exercise
// * If you have not already done so, Create a local *virtualenv* on your workstation, install the *kfp-tekton* package
// +
// [source,bash]
// ----
// $ mkdir kfp #create a folder anywhere
// $ cd kfp
// $ python3 -m venv .venv
// $ source .venv/bin/activate # activate the venv
// $(.venv) pip install kfp-tekton~=1.5.0
// ----
// +
// IMPORTANT: Using the correct Python module versions is critical to avoid conflicts between the KFP SDK and Data Science Pipelines versions. Install the latest *kfp-tekton* module version 1.5.x. Installing the 1.8.x versions will result in failures during pipeline runs.

// We're now going to deploy and run a sample pipeline. The use case is identifying fraudulent transactions among a large number of credit card transactions using an offline scoring approach.

// === Setup

// . If you have not already done so, create a new S3 Bucket named *fraud-detection* in the Minio web console (You should have done this in the previous exercise on Elyra pipelines).
// +
// image::create-s3-fraud-bucket.png[title=Create a new S3 bucket]

// . If you have not already done so, download the following artifacts:

// * xref:attachment$model-latest.onnx[ONNX Model File]
// * xref:attachment$live-data.csv[Credit Card Transaction Data]
 
// . If you have not already done so, upload the files to the S3 *fraud-detection* bucket using the Minio web console.
// +

// image::fraud-upload-files.png[title=Upload files to fraud-detection S3 Bucket]
// +
// image::fraud-files-uploaded.png[title=S3 Bucket Contents after upload]

// . If you have not already done so, log in to the OpenShift cluster using the `oc` CLI as the `admin` user, and create the *aws-connection-fraud-detection* secret containing the S3 bucket name, endpoint and credentials.
// +
// [source,bash]
// ----
// $ oc create secret \
//   generic aws-connection-fraud-detection \
//   --from-literal=AWS_ACCESS_KEY_ID=minio \
//   --from-literal=AWS_S3_BUCKET=fraud-detection \
//   --from-literal=AWS_S3_ENDPOINT=http://minio-service.pipelines-example.svc:9000 \
//   --from-literal=AWS_SECRET_ACCESS_KEY=minio123 \
//   -n pipelines-example
// ----

// . If you have not already done so, create two OpenShift Persistent Volume Claims (PVC) with the following names. Download the YAML files in the links below:
// +
// NOTE: You may have already created the *offline-scoring-data-volume* PVC in the previous Elyra pipelines section. In this scenario, create only the *offline-scoring-model-volume* PVC below.
// +
// * *offline-scoring-model-volume* (xref:attachment$offline-scoring-data-pvc.yaml[Data Volume definition])
// * *offline-scoring-data-volume* (xref:attachment$offline-scoring-model-pvc.yaml[Model Volume definition])

// . Log in to the OpenShift cluster as the `admin` user and create the two PVCs:
// +
// ```bash
// $ oc apply -f offline-scoring-data-pvc.yaml -n pipelines-example
// $ oc apply -f offline-scoring-model-pvc.yaml -n pipelines-example
// ```

// . Verify that you see the two PVCs by navigating to `Storage > PersistentVolumeClaims` in the OpenShift web console (`Administrator` perspective).
// +
// image::fraud-demo-pvcs.png[title=PVCs for Demo]

// === Execute Pipeline

// . Download and inspect the xref:attachment$offline_scoring_kfp_pipeline.py[Credit Card Fraud Pipeline definition] Python file.

// . In the `pipelines-example` DS project, create a new `Workbench` with the following options:
// +
// * *Name*: `fraud-onnx`
// * *Image selection*: `Standard Data Science`
// * *Version selection*: Latest image marked `Recommended`
// * *Container size*: `Small`
// * *Cluster Storage*: Create new persistent storage with name `fraud-onnx`
// * *Persistent storage size*: `5GB`
// * *Data connections*: Use existing data connection `fraud-detection`
// +
// image::fraud-onnx-workbench.png[title=Create a new workbench]

// . Open the `fraud-onnx` workbench from your RHOAI dashboard. Create a new notebook named `fraud-detection-kfp` and copy the code from the `offline_scoring_kfp_pipeline.py` file into the notebook. Save the changes to the notebook file.

// . Click 'Restart Kernel and Run All Cells' in the Notebook.
// +
// image::start-pipeline.png[title=Start Pipeline Execution]
// +

// . The Pipeline will be submitted to the server and a run _Triggered_.
// +
// image::pipeline-run-triggered.png[title=Pipeline Triggered]

// . If the pipeline run was executed without any issues, you should see something similar to below:
// +
// image::pipeline-finish.png[title=Successful Pipeline Execution]

// === Handling Pipeline Errors

// . In case the pipeline fails, the UI shows something similar to the following:
// +
// image::pipeline-failure.png[title=Pipeline Execution Failure]

// . Clicking on the *_failed_* node in the graph and then clicking on the _Details_ tab presents the status of the execution step. 
// When the mouse is placed over the *_Status_* field, the command to retrieve the detailed logs of the execution step is displayed.
// +
// image::pipeline-error-details.png[title=Pipeline Error Details]

// . The step logs can then be viewed using the command provided.
// +
// image::pipeline-pod-error.png[title=Pipeline Execution Step Logs]