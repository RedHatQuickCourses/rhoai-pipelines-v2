= Kubeflow Pipelines SDK

In the previous section, we created a two-task pipeline by using the Elyra GUI pipeline editor. The kfp SDK provides a Python API for creating pipelines. The SDK is available as a Python package that you can install by using the `pip install kfp` command. With this package, you can use Python code to create a pipeline and then compile it to YAML format. Then we can import the YAML code into OpenShift AI.

This course does not delve into the details of how to use the SDK. Instead, it provides the files for you to view and upload.


== Lab Exercise: Importing a Data Science Pipeline

=== Prerequisites 

* Continue to use the `fraud-detection` Data Science Project that you created in the previous section. We won't need a workbench in this section, but you should have completed all lab exercises up through the Data Science Pipelines / OpenShift AI Resources section of the course in order for the environment to support pipeline creation.

image::import_pipeline_yaml.gif[width=600]


. In the RHOAI side navigation menu, click `Data Science Pipelines

. Select the import pipeline button

. For the pipeline name use: *fraud-detection-example*

. Select the link below to download the fraud-detection.yaml file to import.
+
https://github.com/RedHatQuickCourses/rhoai-pipelines-v2/blob/main/downloads/fraud_detection.yaml[Fraud Detection Pipeline Definition Yaml, window=blank]

. Use the download arrow to save the file locally.
+
image::download_pipeline_yaml.png[width=600]

. Move back to the pipeline import page, and select the 'upload a file' Pipeline radial button.

. Click on Upload

. Browse to the folder where you downloaded the fraud-detection.yaml file, and select the file, then open to import the pipeline definition.

.. Optionally, it is possible to import the pipeline from a URL.

Once the pipeline definition is imported, it can be viewed in the data science pipeline dashboard.  

There are three tabs in the DSP dashboard:

 * Graph - the DAG view of the pipeline defined
 * Summary - shows the pipeline spec and version IDs
 * Pipeline spec - is the import YAML file that defines the pipeline.

The pipeline is now available to be executed, but currently, there have been no *one-off or scheduled runs* for this pipeline, it has only been defined in the system. 


//In the Minio web console, click `Object Browser > data-science-pipelines > artifacts > PIPELINE_NAME-XXX`, where `xxxxx` is a randomly generated number for the pipeline run. You should the output artifacts generated by the pipeline.

// image::object-store-after-run.png[]

=== Experiments And Runs

An experiment is a workspace where you can try different configurations of your pipelines. You can use experiments to organize pipeline runs into logical groups. Experiments can contain arbitrary runs, including recurring runs. 

A run can be configured using the Data Science Pipeline UI or programmatically using the _KFP SDK_.

image::create_pipeline_run.gif[width=600]

To create a _run_ for the fraud-detection-example pipeline we just imported.

 . Locate the fraud-detection-example in the data science pipeline menu/dashboard.

 . Use the menu at the far right to:

  .. upload a new version of the pipeline
  .. create a schedule run for some time in the future
  .. create a one-off run

 . Select the option to create a new run.

To execute the run, we need to input some information:

 . Run Type: Scheduled runs are executed from a different dashboard - skip this step

 . Define the project and experiment name:

 .. The project name is immutable and cannot be changed at this time.

 .. For the experiment, choose an existing experiment from the list or create a new one.  Select create a new experiment. 

 . Run Details:  
 
 .. Specify a name for this run

 .. Add a description

The final section describes the pipeline 

 . Select the pipeline to Run from the drop-down

 . Select the version of the pipeline if available.

Depending on the Pipeline Definition, some parameters must be specified at runtime.

In this case, there are two required parameters, which allow this pipeline to have different inputs.

 . The first parameter is a URL location of the data file to be imported during the Run.

```yaml
{"url": "https://raw.githubusercontent.com/rh-aiservices-bu/fraud-detection/main/data/card_transdata.csv"}
```

 . The second parameter is the number of epochs.
```yaml
{"epochs": 2}
```


[NOTE]
This epoch's number is an important hyperparameter for the algorithm. It specifies the number of epochs or complete passes of the entire training dataset passing through the training or learning process of the algorithm.

Select _Create_ Run to start the run.


== View the Running Pipeline Details

image::running_pipeline.gif[width=600]

 . Select the Experiments and Runs menu.
 
 . In the `experiments and runs` menu, select the fraud-detection-example pipeline experiment.

 * In the window that opens all runs under this experiment are shown, currently, there is only a single run.

 . Selecting the *fraud-detection-example run* will open a run-specific window showing the graph, details, and pipeline spec tabs which provide information about the run.

 * In the graph view, each task will indicate the progress: running, completed, or failed.

 ** Selecting a task, will display information about the task including:

 *** input /output 
 *** Task details
 *** Volumes
 *** Logs

 ** Access to the above tabs is limited until the pipeline run completes or fails.

 . Select the Executions menu.

 ** This page displays each running task for all pipelines along with the task status.

 ** No additional information is shown on this page, refer to the graph page for a detailed status overview of each task. 

 . Select the Artifact menu.

 ** This view displays any artifacts created by all pipeline runs.

 *** A Type of artifact is displayed such as Datasets and models.

 *** A URI to the S3 storage location of the artifact.

 . Once the pipeline completes:

 ** If metrics have been configured they will be available from the Experiments and Runs menu.

 *** In our example, metrics are provided for Accuracy and Previous Model Accuracy. 

image::pipeline_metrics.gif[width=600]

== Next Steps

There are additional pipeline examples listed in the Appendix section. If you have a pipeline example you would like to share, please submit an issue, and let's get it added.

Good luck in your pipeline adventures.