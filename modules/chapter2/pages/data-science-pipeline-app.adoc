= Data Science Pipeline Applications

The *DataSciencePipelineApplication* (dspa) custom resource creates several pods that are necessary to utilize the tooling.  This includes the creation of an API endpoint and a database where metadata is stored.  The API endpoint is used by the OpenShift AI Dashboard, as well as tools like *Elyra* and the *kfp* package to manage and execute pipelines.

image::dpsa_services.gif[width=600]

In Red Hat OpenShift AI, the Data Science Pipeline runtime consists of the following components:

* A Data Science Pipeline Server container.
* A MariaDB database for storing pipeline definitions and results.
* A Pipeline scheduler for scheduling pipeline runs.
* Metadata envoy and grpc pods
* A workflow controller for dspa
* A persistent agent to record the set of runtime containers that are executed as well as their inputs and outputs.

Additionally, the *DataSciencePipelineApplication* requires an S3-compatible storage solution to store artifacts that are generated in the pipeline.

[NOTE]
====
Any S3-compatible storage solution can be used for Data Science Pipelines, including AWS S3, OpenShift Data Foundation, or Minio. In this course, we will use Minio as it is a lightweight and easy to deploy S3 storage solution. Red Hat recommends OpenShift Data Foundation in scenarios where security, data resilience, and disaster recovery are important concerns.
====

== Multi-Tenancy with Data Science Pipeline Applications

As previously mentioned, Data Science Pipelines is designed to be a secure multi-tenant solution.  This means that multiple users and teams can all securely use their own instances of Data Science Pipelines without fear of leaking data from the pipelines to other users or groups.

This multi-tenancy capability does require that each user or group needs their own instance of the *DataSciencePipelineApplication* instance.  Additionally, it is strongly recommended that each *DataSciencePipelineApplication* instance should have its own S3 instance that does not allow other groups to access it.

While a *DataSciencePipelineApplication* is a namespace scoped object, workbenches and pods running in other namespaces can still interact with the pipeline instance if they have the correct permissions.

Only one dpsa deployment can exist per data science project. (namespace)

== Lab Exercise: Create a Data Science Pipeline Instance

To begin we will use the *Minio* instance created with the lab environment to act as the S3 artifact storage for the *DataSciencePipelineApplication*.  


=== Create Data Connections in Data Science Project

Next, we will create a data connection for the Minio instance, and use that data connection to deploy a *DataSciencePipelineApplication*.

image::data_connection_setup.gif[width=600]

. From the OpenShift AI Dashboard, navigate to the `fraud-detection` project we previously created.  Click on the option to `Add data connection`.


. Enter the following details and click `Add data connections`:
+
```
Name: pipelines
Access key: minio
Secret key: minio321!
Endpoint: http://minio-service.pipelines-example.svc:9000
Region: us-east-1 (when using minio this value can be any text but not blank)
Bucket: pipelines
```
+
```
Name: my-storage
Access key: minio
Secret key: minio321!
Endpoint: http://minio-service.pipelines-example.svc:9000
Region: no-region-minio (when using minio this value can be any text but not blank)
Bucket: storage
```

[TIP]
====
A `Data Connection` is simply a standard kubernetes secret object that contains the fields required to connect to an S3 compatible solution.  This secret can be managed via GitOps just like any other standard kubernetes secret object.  However, not all fields in the Data Connection are dynamically consumed by the *DataSciencePipelineApplication* object, so be careful when updating the endpoint URL or the bucket values.
====


image::pipeline_server_setup.gif[width=600]

=== Create a Data Science Pipeline Application

. A new data connection should now be listed in the `data connections` section.

. Switch to the pipelines tab in the data science project.

. Click on the `configure pipeline server` in the `pipelines` section of the data science project view.

. Click the key icon on the right side of the `Access Key` field, and select the `pipelines` data connection. The fields in the form are automatically populated.

. Select the option to use the default database stored in the cluster.

.. There is an option to specify the details of an external database required for the datasciencepipelineapplication.

. Click `configure pipeline server`. After several seconds, the loading icon should complete and the `pipelines` section will now show an option to `import pipeline`.


The *DataSciencePipelineApplication* has now successfully been configured and is ready for use.

[WARNING]
If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. You must delete the pipeline server and configure another one.

[TIP]
You are not required to specify any storage directories (folders) when configuring a data connection for your pipeline server. When you import a pipeline, the /pipelines folder is created in the root folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the /pipelines folder. When you run a pipeline, the artifacts are stored in the /pipeline-name folder in the root folder of the bucket.

== Managing Permissions to the DataSciencePipelineApplication

The *DataSciencePipelineApplication* API endpoint route is protected using an OpenShift OAuth Proxy sidecar.

The OAuth Proxy requires anything attempting to access the endpoint to be authenticated using the built-in OpenShift login.  OpenShift is then able to admit or reject requests to the endpoint based on the Role Based Access and Control configuration of the resources in the namespace.

[NOTE]
====
To learn more about the OpenShift OAuth Proxy, please refer to the official git repo:

https://github.com/openshift/oauth-proxy[, window=_blank]
====

In particular, the *DataSciencePipelineApplication* requires that users or Service Accounts have `get` access to the *DataSciencePipelineApplication* route object.

Any user that has already been granted `Admin` or `Edit` access to the namespace in which the *DataSciencePipelineApplication* is installed will have permission to access the object.

It may be necessary to grant access to other resources such as a Service Account in the cluster to be able to interact with the API endpoint.

To grant access to an object such as a Service Account, you must first create a role in the namespace (project) where the *DataSciencePipelineApplication* is located that grants `get` access to the route object:

```
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dspa-access
  namespace: my-project
rules:
  - verbs:
      - get
    apiGroups:
      - route.openshift.io
    resources:
      - routes
```

Once the role has been created, a `RoleBinding` can grant the appropriate permissions to the user or Service Account:

```
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dspa-access-my-service-account
  namespace: my-project
subjects:
  - kind: ServiceAccount
    name: my-service-account
    namespace: my-project
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dspa-access
```

When programmatically accessing the API endpoint, a user can authenticate to the endpoint by passing the `BearerToken` header value in the http request.  Users can obtain their bearer token from the `Copy Login Command` menu option in the OpenShift Web Console, or by running the following command once they are already logged in:

```bash
$ oc whoami --show-token
```

Using the bearer token to authenticate to the endpoint will be discussed in more detail in the section discussing the `Kubeflow Pipelines SDK`.