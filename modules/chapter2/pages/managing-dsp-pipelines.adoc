= RHOAI Pipeline Overview

== Managing Data Science Pipelines 2.0 

=== Configuring a pipeline server

Before you can successfully create a pipeline in OpenShift AI, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.
 
 * You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account. 
 * You have created a data science project that you can add a pipeline server to.
 * By default in RHOAI when a pipeline server is created it deploys a namespace mariaDB.
 * If you are configuring a pipeline server with an external database.
 ** Red Hat recommends that you use MySQL version 8.x.
 ** Red Hat recommends that you use at least MariaDB version 10.5.

=== Defining a  Pipeline 2.0 
Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. After defining the pipeline, you can import the YAML file to the OpenShift AI dashboard to enable you to configure its execution settings.

You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information about creating pipelines in JupyterLab, see Creating Pipelines in Elyra in the section below. For more information about the Elyra JupyterLab extension, https://elyra.readthedocs.io/en/v2.0.0/getting_started/overview.html[see Elyra Documentation.]


=== Importing a  Pipeline 2.0 
To help you begin working with data science pipelines in OpenShift AI, you can import a YAML file containing your pipelineâ€™s code to an active pipeline server, or you can import the YAML file from a URL.
This file contains a Kubeflow pipeline compiled by using the Kubeflow compiler. After you have imported the pipeline to a pipeline server, you can execute the pipeline by creating a pipeline run.


=== Pipeline Actions

OpenShift AI data science pipelines supports the following actions:

 . Creating
 . Scheduling
 . Executing
 . Viewing
 . Archiving
 . Restoring
 . Deleting
 . Stopping
 . Duplicating

=== Working with pipeline logs
You can review and analyze step logs for each step in a triggered pipeline run.
To help you troubleshoot and audit your pipelines, you can review and analyze these step logs by using the log viewer in the OpenShift AI dashboard. 

 * Viewing logs
 * Downloading logs

