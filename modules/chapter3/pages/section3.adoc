= OpenShift AI Resources

//video::llm_dataconn_v3.mp4[width=640]

== Configure the OpenShift AI Data Science Cluster

video::llm_dsc_v3.mp4[width=640]

== Create OpenShift AI Data Science Cluster

With our secrets in place, the next step is to create an OpenShift AI *Data Science Cluster*.

_A DataScienceCluster is the plan in the form of an YAML outline for Data Science Cluster API deployment._

Return to the OpenShift Navigation Menu, Select Installed Operators, and Click on the OpenShift AI Operator name to open the operator.

 . *Select the Option to create a Data Science Cluster.*

 . *Select the radial button to switch to the YAML view.*

 . Find the section below in the YAML file, in the Kserve Section find the Serving/Certificate area; add the line: *secretName:* followed by the name of the secret name that we deployed in the istio-system project. In addition, change the type from SelfSigned to *Provided*. See below for the example.

```yaml
kserve:
devFlags: {}
managementState: Managed
serving:
    ingressGateway:
    certificate:
        secretName: cert-manager-ingress-cert
        type: Provided
    managementState: Managed
    name: knative-serving
```
image::dsc_cert_example.png[width=640]

Once you have made those changes to the YAML file, *Click Create* to Deploy the Data Science Cluster.  

image::dsc_deploy_complete.png[width=640]

Single Model Serve Platform will now be deployed to expose ingress connections with the same certificate as OpenShift Routes. Endpoints will be accessible using TLS without having to ignore error messages or create special configurations.

== OpenShift AI install summary

Congratulations, you have successfully completed the installation of OpenShift AI on an OpenShift Container Cluster. OpenShift AI is now running on a new Dashboard!


  * We installed the required OpenShift AI Operators
  ** Red Hat OpenShift Serverless 
  ** Red Hat OpenShift ServiceMesh
  ** Red Hat Authorino (technical preview)
  ** OpenShift AI Operator




== Create a Data Science Project 

Navigate to & select the Data Science Projects section.

 . Select the create data science project button.

 . Enter a name for your project, such as *ollama-model*.

 . The resource name should be populated automatically.

 . Optionally add a description to the data science project.

 . Select Create.

//image::dsp_create.png[width=640]

 
The next step is to create a *Data Connection* in our Data Science Project.  Before we can create our Data Connection, we will setup MinIO as our S3 compatible storage for this Lab. 

Continue to the next section to deploy and configure Minio. 

== Create Data Connection 

Navigate to the Data Science Project section of the OpenShift AI Console /Dashboard. Select the Ollama-model project. 

. Select the Data Connection menu, followed by create data connection
. Provide the following values:
..  Name:  *models*
..  Access Key: use the minio_root-user from YAML file
..  Secret Key: use the minio_root_password from the YAML File
..  Endpoint: use the Minio API URL from the Routes page in Openshift Dashboard
..  Region: This is required for AWS storage & cannot be blank (no-region-minio)
.. Bucket: use the Minio Storage bucket name: *models* 

//image::dataconnection_models.png[width=800]

Repeat the same process for the Storage bucket, using *storage* for the name & bucket.

== Creating a WorkBench 

//video::openshiftai_setup_part3.mp4[width=640]

Navigate to the Data Science Project section of the OpenShift AI Console /Dashboard. Select the Ollama-model project.  

//image::create_workbench.png[width=640]

 . Select the WorkBench button, then click create workbench

 .. Name:  `tbd`

 .. Notebook Image:  `Minimal Python`

 .. Leave the remaining options default.

 .. Optionally, scroll to the bottom, check the `Use data connection box`.
 
 .. Select *storage* from the dropdown to attach the storage bucket to the workbench.  

 . Select the Create Workbench option.

[NOTE]
Depending on the notebook image selected, it can take between 2-20 minutes for the container image to be fully deployed. The Open Link will be available when our container is fully deployed.  



= Jupyter Notebooks

// video::llm_jupyter_v3.mp4[width=640]

== Open JupyterLab 

JupyterLab enables you to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner. For a demonstration of JupyterLab and its features, https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html#what-will-happen-to-the-classic-notebook[you can view this video.]


Return to the ollama-model workbench dashboard in the OpenShift AI console.

 . Select the *Open* link to the right of the status section.
+
image::oai_open_jupyter.png[width=640]

 . When the new window opens, use the OpenShift admin user & password to login to JupyterLab. 

 . Click the *Allow selected permissions* button to complete login to the notebook.


[NOTE]
If the *OPEN* link for the notebook is grayed out, the notebook container is still starting. This process can take a few minutes & up to 20+ minutes depending on the notebook image we opted to choose.


== Inside JupyterLab

This takes us to the JupyterLab screen where we can select multiple options / tools / to work to begin our data science experimentation.

Our first action is to clone a git repository that contains a collection of LLM projects including  the notebook we are going to use to interact with the LLM. 

Clone the github repository to interact with the Ollama Framework from this location:
https://github.com/rh-aiservices-bu/llm-on-openshift.git

 . Copy the URL link above

 . Click on the Clone a Repo Icon above explorer section window.
+
image::clone_a_repo.png[width=640]

 . Paste the link into the *clone a repo* pop up,   make sure the *included submodules are checked*, then click the clone.
 
 . Navigate to the llm-on-openshift/examples/notebooks/langchain folder:

 . Then open the file: _Langchain-Ollama-Prompt-memory.ipynb_
+
image::navigate_ollama_notebook.png[width=640]

 . Explore the notebook, and then continue.
